{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Big Data Project"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Intialising Spark and loading Data"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": "# The code was removed by Watson Studio for sharing."
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": "sc = spark.sparkContext"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": "#Libraries\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import *\nfrom pyspark.sql.types import FloatType\nfrom pyspark.ml.feature import VectorAssembler, VectorIndexer, StandardScaler, OneHotEncoder, Imputer, StringIndexer\nfrom pyspark.ml import feature\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n#Other Libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport time\nimport matplotlib.pyplot as plt"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "2260668"
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "df.count()"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "# These columns will be unique to each customer\ndf = df.drop('id', 'member_id')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "id \t with null values:  2260668\n"
                }
            ],
            "source": "# Looping through to check for null values \nfor col in df.columns:\n    cnt = df.filter(df[col].isNull()).count()\n    if cnt != 0:\n      print(col, \"\\t\", \"with null values: \", cnt)   "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df.groupBy(['loan_status']).count().orderBy('count', ascending = False).show(10,  truncate = False)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df = df.where(~df.loan_status.isin(['Oct-2015']))\ndf = df.withColumn(\"Default\", \n   F.when((df.loan_status == 'Fully Paid') | (df.loan_status == 'Current') | (df.loan_status == 'Does not meet the credit policy. Status:Fully Paid') ,0).otherwise(1))\n\ndf = df1.drop('loan_status')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df.groupBy(['Default']).count().orderBy('count', ascending = False).show(10,  truncate = False)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Data is Skewed by 1 : 6.67, hence AUC will be used as Evaluvation Metrics"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df.groupBy(['emp_length']).count().show(15,  truncate = False)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# ['10+ years', '< 1 year', '1 year', '3 years', '8 years', '9 years', '4 years', '5 years', '6 years', '2 years', '7 years', 'n/a' ]\ndf1 = df.where(~df.emp_length.isin([' reactors\"']))\ndf1 = df1.fillna('U', subset = ['emp_length'])\ndf1 = df1.withColumn(\"employment_length\", \n   F.when((df.emp_length == 'U') ,0).\n                     F.when((df.emp_length == '< 1 year') ,1).\n                     F.when((df.emp_length == '1 year') ,2).\n                     F.when((df.emp_length == '2 year') ,3).\n                     F.when((df.emp_length == '3 year') ,4).\n                     F.when((df.emp_length == '4 year') ,5).\n                     F.when((df.emp_length == '5 year') ,6).\n                     F.when((df.emp_length == '6 year') ,7).\n                     F.when((df.emp_length == '7 year') ,8).\n                     F.when((df.emp_length == '8 year') ,9).\n                     F.when((df.emp_length == '9 year') ,10).otherwise(11))\n\n\ndf1 = df1.drop('emp_length')\ndf1.groupBy(['empoyment_length']).count().show(15,  truncate = False)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df1.groupBy(['verification_status']).count().show(truncate = False)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df1 = df1.where(~df.verification_status.isin(['38000']))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "us_states = ['CA', 'OR', 'UT','WA', 'CO', 'NV', 'AK', 'MT', 'HI', 'WY', 'ID', 'AZ', 'TX', 'NM', 'OK', 'GA', 'NC', 'VA', 'FL', 'KY',\\\n             'SC', 'LA', 'AL', 'WV', 'DC', 'AR', 'DE', 'MS', 'TN', 'IL', 'MO', 'MN', 'OH', 'WI', 'KS', 'MI', 'SD', 'IA', 'NE', 'IN',\\\n             'ND','CT', 'NY', 'PA', 'NJ', 'RI','MA', 'MD', 'VT', 'NH', 'ME']\n\ndf2 = df1.where(df.addr_state.isin(us_states))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df2.count()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6 with Spark",
            "language": "python3",
            "name": "python36"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}